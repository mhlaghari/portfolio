{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Part 1-Pyspark DataFrames\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trips_df = spark.read.csv('gs://mhl-exam/data.loans2 (1).csv',header=True, inferSchema=True )mhl-exam/data/loans2 (1).csv"]},{"cell_type":"code","execution_count":2,"metadata":{"scrolled":true},"outputs":[{"data":{"text/plain":["Row(VendorID=1, tpep_pickup_datetime=datetime.datetime(2020, 1, 1, 0, 28, 15), tpep_dropoff_datetime=datetime.datetime(2020, 1, 1, 0, 33, 3), passenger_count=1, trip_distance=1.2, RatecodeID=1, store_and_fwd_flag='N', PULocationID=238, DOLocationID=239, payment_type=1, fare_amount=6.0, extra=3.0, mta_tax=0.5, tip_amount=1.47, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=11.27, congestion_surcharge=2.5)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["trips_df.head()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n","|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n","+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n","|       1| 2020-01-01 00:28:15|  2020-01-01 00:33:03|              1|          1.2|         1|                 N|         238|         239|           1|        6.0|  3.0|    0.5|      1.47|         0.0|                  0.3|       11.27|                 2.5|\n","|       1| 2020-01-01 00:35:39|  2020-01-01 00:43:04|              1|          1.2|         1|                 N|         239|         238|           1|        7.0|  3.0|    0.5|       1.5|         0.0|                  0.3|        12.3|                 2.5|\n","|       1| 2020-01-01 00:47:41|  2020-01-01 00:53:52|              1|          0.6|         1|                 N|         238|         238|           1|        6.0|  3.0|    0.5|       1.0|         0.0|                  0.3|        10.8|                 2.5|\n","|       1| 2020-01-01 00:55:23|  2020-01-01 01:00:14|              1|          0.8|         1|                 N|         238|         151|           1|        5.5|  0.5|    0.5|      1.36|         0.0|                  0.3|        8.16|                 0.0|\n","|       2| 2020-01-01 00:01:58|  2020-01-01 00:04:16|              1|          0.0|         1|                 N|         193|         193|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         4.8|                 0.0|\n","|       2| 2020-01-01 00:09:44|  2020-01-01 00:10:37|              1|         0.03|         1|                 N|           7|         193|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|\n","|       2| 2020-01-01 00:39:25|  2020-01-01 00:39:29|              1|          0.0|         1|                 N|         193|         193|           1|        2.5|  0.5|    0.5|      0.01|         0.0|                  0.3|        3.81|                 0.0|\n","|       2| 2019-12-18 15:27:49|  2019-12-18 15:28:59|              1|          0.0|         5|                 N|         193|         193|           1|       0.01|  0.0|    0.0|       0.0|         0.0|                  0.3|        2.81|                 2.5|\n","|       2| 2019-12-18 15:30:35|  2019-12-18 15:31:35|              4|          0.0|         1|                 N|         193|         193|           1|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         6.3|                 2.5|\n","|       1| 2020-01-01 00:29:01|  2020-01-01 00:40:28|              2|          0.7|         1|                 N|         246|          48|           1|        8.0|  3.0|    0.5|      2.35|         0.0|                  0.3|       14.15|                 2.5|\n","|       1| 2020-01-01 00:55:11|  2020-01-01 01:12:03|              2|          2.4|         1|                 N|         246|          79|           1|       12.0|  3.0|    0.5|      1.75|         0.0|                  0.3|       17.55|                 2.5|\n","|       1| 2020-01-01 00:37:15|  2020-01-01 00:51:41|              1|          0.8|         1|                 N|         163|         161|           2|        9.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        13.3|                 2.5|\n","|       1| 2020-01-01 00:56:27|  2020-01-01 01:21:44|              1|          3.3|         1|                 N|         161|         144|           1|       17.0|  3.0|    0.5|      4.15|         0.0|                  0.3|       24.95|                 2.5|\n","|       2| 2020-01-01 00:21:54|  2020-01-01 00:27:31|              1|         1.07|         1|                 N|          43|         239|           1|        6.0|  0.5|    0.5|      1.96|         0.0|                  0.3|       11.76|                 2.5|\n","|       2| 2020-01-01 00:38:01|  2020-01-01 01:15:21|              1|         7.76|         1|                 N|         143|          25|           1|       28.5|  0.5|    0.5|      4.84|         0.0|                  0.3|       37.14|                 2.5|\n","|       1| 2020-01-01 00:15:35|  2020-01-01 00:27:06|              3|          1.6|         1|                 N|         211|         234|           2|        9.0|  3.0|    0.5|       0.0|         0.0|                  0.3|        12.8|                 2.5|\n","|       1| 2020-01-01 00:41:20|  2020-01-01 00:44:22|              1|          0.5|         1|                 Y|         234|          90|           1|        4.0|  3.0|    0.5|       1.0|         0.0|                  0.3|         8.8|                 2.5|\n","|       1| 2020-01-01 00:56:38|  2020-01-01 01:13:34|              1|          1.7|         1|                 N|         246|         142|           2|       11.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        15.3|                 2.5|\n","|       2| 2020-01-01 00:08:21|  2020-01-01 00:25:29|              1|         8.45|         1|                 N|         138|         216|           2|       24.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        25.8|                 0.0|\n","|       1| 2020-01-01 00:25:39|  2020-01-01 00:27:05|              1|          0.0|         1|                 N|         170|         162|           4|        3.0|  3.0|    0.5|       0.0|         0.0|                  0.3|         6.8|                 2.5|\n","+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["trips_df.show(20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Question 1"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------+\n","|avg(trip_distance)|\n","+------------------+\n","| 2.897359856402041|\n","+------------------+\n","\n"]}],"source":["trips_df.agg({'trip_distance': 'mean'}).show( )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Question 2"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------+\n","|avg(trip_distance)|\n","+------------------+\n","|2.9716795553859203|\n","+------------------+\n","\n"]}],"source":["trips_df.filter(trips_df.passenger_count == 2).agg({'trip_distance': 'mean'}).show( )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Question 3"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["298564"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["trips_df.filter(trips_df.RatecodeID == 2).count()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Question 4"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------+\n","|  avg(fare_amount)|\n","+------------------+\n","|57.875639002215934|\n","+------------------+\n","\n"]}],"source":["fare = trips_df.filter(trips_df.RatecodeID == 5).agg({'fare_amount': 'mean'}).show()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------+------------------+\n","|passenger_count|avg(date_diff_min)|\n","+---------------+------------------+\n","|           null|28.425607671552477|\n","|              1|15.532790958818948|\n","|              6|19.028427166873687|\n","|              3|17.459121473716532|\n","|              5|19.480482677562552|\n","|              9| 8.610493827160495|\n","|              4|17.449907643622797|\n","|              8|13.179047619047621|\n","|              7| 11.49060606060606|\n","|              2|16.361562048705718|\n","|              0|12.717738262886133|\n","+---------------+------------------+\n","\n"]}],"source":["import pyspark.sql.functions as F\n","trips_new= trips_df.withColumn(\n","    \"date_diff_min\", \n","    (F.col(\"tpep_dropoff_datetime\").cast(\"long\") - F.col(\"tpep_pickup_datetime\").cast(\"long\"))/60.\n",")\n","trips_new.groupby('passenger_count').agg({'date_diff_min': 'mean'}).show()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+------------------+\n","|RatecodeID|        avg(speed)|\n","+----------+------------------+\n","|      null|21.067977779855404|\n","|         1| 11.71248800540977|\n","|         6|317.85680681990516|\n","|         3| 53.24696907482183|\n","|         5| 77.86570552049292|\n","|         4| 45.09750559533565|\n","|         2| 34.46011888837545|\n","|        99| 7.502451564689387|\n","+----------+------------------+\n","\n"]}],"source":["trips_new2= trips_df.withColumn(\n","    \"date_diff_hour\", \n","   (F.col(\"tpep_dropoff_datetime\").cast(\"long\") - F.col(\"tpep_pickup_datetime\").cast(\"long\"))/3600.\n",")\n","trips_new3= trips_new2.withColumn('speed', trips_new2.trip_distance/trips_new2.date_diff_hour)\n","trips_new3.groupby('RatecodeID').agg({'speed': 'mean'}).show()"]},{"cell_type":"markdown","metadata":{},"source":["# Part 2 - Pyspark Machine Learning"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["loans = spark.read.csv('gs://mhl-exam/data/loans2 (1).csv',header=True, inferSchema=True )"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["614"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["loans.count()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["Row(Loan_ID='LP001002', Gender='Male', Married='No', Dependents='0', Education='Graduate', Self_Employed='No', ApplicantIncome=5849, CoapplicantIncome=0.0, LoanAmount=None, Loan_Amount_Term=360, Credit_History=1, Property_Area='Urban', Loan_Status='Y')"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["loans.head()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["DataFrame[Loan_ID: string, Gender: string, Married: string, Dependents: string, Education: string, Self_Employed: string, ApplicantIncome: int, CoapplicantIncome: double, LoanAmount: int, Loan_Amount_Term: int, Credit_History: int, Property_Area: string, Loan_Status: string]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["loans"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["df = loans.dropna()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+------+-------+----------+------------+-------------+---------------+-----------------+----------+----------------+--------------+-------------+-----------+\n","| Loan_ID|Gender|Married|Dependents|   Education|Self_Employed|ApplicantIncome|CoapplicantIncome|LoanAmount|Loan_Amount_Term|Credit_History|Property_Area|Loan_Status|\n","+--------+------+-------+----------+------------+-------------+---------------+-----------------+----------+----------------+--------------+-------------+-----------+\n","|LP001003|  Male|    Yes|         1|    Graduate|           No|           4583|           1508.0|       128|             360|             1|        Rural|          N|\n","|LP001005|  Male|    Yes|         0|    Graduate|          Yes|           3000|              0.0|        66|             360|             1|        Urban|          Y|\n","|LP001006|  Male|    Yes|         0|Not Graduate|           No|           2583|           2358.0|       120|             360|             1|        Urban|          Y|\n","+--------+------+-------+----------+------------+-------------+---------------+-----------------+----------+----------------+--------------+-------------+-----------+\n","only showing top 3 rows\n","\n"]}],"source":["df.show(3)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+------+-------+----------+------------+-------------+----------+----------------+--------------+-------------+-----------+------------+\n","| Loan_ID|Gender|Married|Dependents|   Education|Self_Employed|LoanAmount|Loan_Amount_Term|Credit_History|Property_Area|Loan_Status|total_income|\n","+--------+------+-------+----------+------------+-------------+----------+----------------+--------------+-------------+-----------+------------+\n","|LP001003|  Male|    Yes|         1|    Graduate|           No|       128|             360|             1|        Rural|          N|      6091.0|\n","|LP001005|  Male|    Yes|         0|    Graduate|          Yes|        66|             360|             1|        Urban|          Y|      3000.0|\n","|LP001006|  Male|    Yes|         0|Not Graduate|           No|       120|             360|             1|        Urban|          Y|      4941.0|\n","+--------+------+-------+----------+------------+-------------+----------+----------------+--------------+-------------+-----------+------------+\n","only showing top 3 rows\n","\n"]}],"source":["df = df.withColumn('total_income',(df.ApplicantIncome+df.CoapplicantIncome)).drop('ApplicantIncome','CoapplicantIncome')\n","df.show(3)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------+\n","|avg(total_income)|\n","+-----------------+\n","|6945.324833309667|\n","+-----------------+\n","\n"]}],"source":["df.agg({'total_income': 'mean'}).show()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["from pyspark.ml.feature import StringIndexer"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"ename":"IllegalArgumentException","evalue":"'requirement failed: Output column Gender_idx already exists.'","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 328\u001B[0;31m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o348.fit.\n: java.lang.IllegalArgumentException: requirement failed: Output column Gender_idx already exists.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:92)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:83)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:109)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:152)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:135)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n","\nDuring handling of the above exception, another exception occurred:\n","\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)","\u001B[0;32m<ipython-input-23-1cda512546f7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStringIndexer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Gender'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0moutputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Gender_idx'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mindexer_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mindexer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mindexer_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStringIndexer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Married'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0moutputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Married_idx'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mindexer_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mindexer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    130\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    131\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 132\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    133\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    134\u001B[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n","\u001B[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    293\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    294\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 295\u001B[0;31m         \u001B[0mjava_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    296\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    297\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_copyValues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    290\u001B[0m         \"\"\"\n\u001B[1;32m    291\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transfer_params_to_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 292\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    293\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    294\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1255\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1256\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1257\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1258\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1259\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m     77\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mQueryExecutionException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m': '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstackTrace\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     78\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'java.lang.IllegalArgumentException: '\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 79\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mIllegalArgumentException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m': '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstackTrace\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     80\u001B[0m             \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     81\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;31mIllegalArgumentException\u001B[0m: 'requirement failed: Output column Gender_idx already exists.'"]}],"source":["indexer = StringIndexer(inputCol='Gender',outputCol='Gender_idx')\n","indexer_model = indexer.fit(df)\n","df = indexer_model.transform(df)\n","indexer = StringIndexer(inputCol='Married',outputCol='Married_idx')\n","indexer_model = indexer.fit(df)\n","df = indexer_model.transform(df)\n","indexer = StringIndexer(inputCol='Education',outputCol='Education_idx')\n","indexer_model = indexer.fit(df)\n","df = indexer_model.transform(df)\n","indexer = StringIndexer(inputCol='Self_Employed',outputCol='Self_Employed_idx')\n","indexer_model = indexer.fit(df)\n","df = indexer_model.transform(df)\n","indexer = StringIndexer(inputCol='Loan_Status',outputCol='Loan_Status_idx')\n","indexer_model = indexer.fit(df)\n","df = indexer_model.transform(df)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+\n","| avg(Education_idx)|\n","+-------------------+\n","|0.20208333333333334|\n","+-------------------+\n","\n"]}],"source":["df.agg({'Education_idx': 'mean'}).show()"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["from pyspark.ml.feature import OneHotEncoderEstimator"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["encoder = OneHotEncoderEstimator(inputCols=['Gender_idx','Married_idx','Education_idx','Self_Employed_idx'], outputCols=[\"Gender_vec\",\"Married_vec\",\"Education_vec\",\"Self_Employed_vec\"])"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["encoder_model = encoder.fit(df)\n","df = encoder_model.transform(df)\n","df = df.withColumnRenamed(\"Loan_Status_idx\",\"label\")"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["from pyspark.ml.feature import VectorAssembler\n","assembler = VectorAssembler(inputCols=['total_income','Gender_vec','Married_vec','Education_vec','Self_Employed_vec'],outputCol='features')\n","df = assembler.transform(df)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"text/plain":["[378, 102]"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["train, test = df.randomSplit([0.8,0.2], seed=2021)\n","[train.count(), test.count()]"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["from pyspark.ml.classification import DecisionTreeClassifier"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+----------+----------------------------------------+-------------------------+\n","|label|prediction|probability                             |features                 |\n","+-----+----------+----------------------------------------+-------------------------+\n","|0.0  |0.0       |[0.7407407407407407,0.25925925925925924]|[6000.0,1.0,0.0,1.0,1.0] |\n","|1.0  |0.0       |[0.7407407407407407,0.25925925925925924]|[4693.0,1.0,0.0,1.0,1.0] |\n","|1.0  |0.0       |[0.6458333333333334,0.3541666666666667] |[7660.0,1.0,1.0,0.0,1.0] |\n","|0.0  |1.0       |[0.1111111111111111,0.8888888888888888] |[11376.0,1.0,0.0,1.0,1.0]|\n","|1.0  |0.0       |[0.6363636363636364,0.36363636363636365]|[3600.0,1.0,0.0,1.0,1.0] |\n","|0.0  |0.0       |[0.7931034482758621,0.20689655172413793]|[6277.0,1.0,1.0,1.0,1.0] |\n","|0.0  |0.0       |[0.7931034482758621,0.20689655172413793]|[5821.0,1.0,1.0,1.0,1.0] |\n","|1.0  |0.0       |[0.7931034482758621,0.20689655172413793]|[11500.0,0.0,1.0,1.0,0.0]|\n","|0.0  |0.0       |[0.7931034482758621,0.20689655172413793]|[10330.0,1.0,1.0,1.0,1.0]|\n","|0.0  |0.0       |[0.7931034482758621,0.20689655172413793]|[6296.0,1.0,1.0,1.0,1.0] |\n","+-----+----------+----------------------------------------+-------------------------+\n","only showing top 10 rows\n","\n"]}],"source":["tree = DecisionTreeClassifier()\n","tree_model = tree.fit(train)\n","prediction = tree_model.transform(test)\n","prediction.select('label','prediction','probability','features').show(10, truncate=False)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+----------+-----+\n","|label|prediction|count|\n","+-----+----------+-----+\n","|  1.0|       1.0|    2|\n","|  0.0|       1.0|    4|\n","|  1.0|       0.0|   32|\n","|  0.0|       0.0|   64|\n","+-----+----------+-----+\n","\n"]}],"source":["prediction.groupBy(\"label\", \"prediction\").count().show()"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy = 0.647059\n","Test Error = 0.352941\n"]}],"source":["evaluator = MulticlassClassificationEvaluator(\n","    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n","accuracy = evaluator.evaluate(prediction)\n","print( \"Accuracy = %g\" % (accuracy))\n","print( \"Test Error = %g\" % (1.0 - accuracy))"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["results = prediction.select(['prediction', 'label'])"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["predictionAndLabels=results.rdd"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Recall for label=1: 0.0588235\n"]}],"source":["from pyspark.mllib.evaluation import MulticlassMetrics\n","metrics = MulticlassMetrics(predictionAndLabels)\n","recall = metrics.recall(label=1);\n","print(\"Recall for label=1: %g\" % recall);"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["weighted Precision = 0.555556\n"]}],"source":["evaluator = MulticlassClassificationEvaluator(\n","    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n","weightedPrecision = evaluator.evaluate(prediction)\n","print( \"weighted Precision = %g\" % (weightedPrecision))"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["weighted Recall = 0.647059\n"]}],"source":["evaluator = MulticlassClassificationEvaluator(\n","    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n","weightedRecall = evaluator.evaluate(prediction)\n","print( \"weighted Recall = %g\" % (weightedRecall))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}